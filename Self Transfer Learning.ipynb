{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model : VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pRelu(x, name='P_ReLU'):\n",
    "    with tf.variable_scope(name):\n",
    "        alpha = tf.get_variable('a', x.get_shape()[-1], initializer=tf.constant_initializer(0.0), dtype=tf.float32)\n",
    "        return tf.maximum(0.0, x) + tf.minimum(0.0, alpha*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def conv2d(input_, output_n, k_h=3, k_w=3, d_h=1, d_w=1, bias=0.0, activation_fc=pRelu, name='conv2d'):\n",
    "    with tf.variable_scope(name):\n",
    "        W = tf.get_variable('W', [k_h, k_w, input_.get_shape()[-1], output_n],\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b = tf.get_variable('b', [output_n], initializer=tf.constant_initializer(bias))\n",
    "        conv = tf.nn.conv2d(input_, W, strides=[1, d_h, d_w, 1], padding='SAME', name='conv')\n",
    "        conv_b = tf.nn.bias_add(conv, b)\n",
    "        act = activation_fc(conv_b, 'act')\n",
    "        \n",
    "        tf.summary.histogram('Weight', W)\n",
    "        tf.summary.histogram('Bias', b)\n",
    "        tf.summary.histogram('Conv', conv)\n",
    "        tf.summary.histogram('Conv_with_bias', conv_b)\n",
    "        tf.summary.histogram('Activation', act)\n",
    "\n",
    "        return act\n",
    "\n",
    "\n",
    "def flatten(input_, name='flatten'):\n",
    "    vec_dim = input_.get_shape()[1:]\n",
    "    n = vec_dim.num_elements()\n",
    "    with tf.name_scope(name):\n",
    "        return tf.reshape(input_, [-1, n])\n",
    "\n",
    "\n",
    "def linear(input_, output_size, stddev=0.02, bias=0.0, name='linear'):\n",
    "    input_size = input_.get_shape().as_list()[1]\n",
    "    with tf.variable_scope(name):\n",
    "        W = tf.get_variable('W', [input_size, output_size], tf.float32,\n",
    "                           initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "        b = tf.get_variable('b', [output_size], initializer=tf.constant_initializer(bias))\n",
    "        logits = tf.nn.xw_plus_b(input_, W, b, name='logits')\n",
    "\n",
    "        tf.summary.histogram('Weight', W)\n",
    "        tf.summary.histogram('Bias', b)\n",
    "        tf.summary.histogram('logit', logits)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "def drop_out(x, prob, name='drop_out'):\n",
    "    with tf.variable_scope(name):\n",
    "        drop_layer = tf.nn.dropout(x, prob)\n",
    "        tf.summary.histogram('Drop_Out', drop_layer)\n",
    "        return drop_layer\n",
    "\n",
    "def wsl_cross_enropy(cls_pred, loc_pred, true, a=0.1):\n",
    "    '''\n",
    "    :param a: Hyperparameter. default = 0.1 and increased to 0.9 after 60 epochs\n",
    "             to determine the level of importance between classifier and localizer.\n",
    "    '''\n",
    "    cls_softmax = tf.nn.softmax(cls_pred, name='Clssification_Softmax')\n",
    "    loc_softmax = tf.nn.softmax(loc_pred, name='Localization_Softmax')\n",
    "\n",
    "    return - (1 - a)*tf.reduce_mean(true * tf.log(cls_softmax)) - a * tf.reduce_mean(true * tf.log(loc_softmax))\n",
    "\n",
    "def batch_norm(x, epsilon=1e-5, momentum=0.9, train=True, name=\"batch_norm\"):\n",
    "    with tf.variable_scope(name):\n",
    "        return tf.contrib.layers.batch_norm(x, decay=momentum,\n",
    "                                            updates_collections=None, epsilon=epsilon, scale=True, scope=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class STL(object):\n",
    "    def __init__(self):\n",
    "        self.output_n = 2\n",
    "        self.class_n = 2\n",
    "        self.default_act_fn = pRelu\n",
    "        print(\"Self-Transfer-Learning is Ready\")\n",
    "    \n",
    "    def conv_layer(self, input_):\n",
    "        self.conv1_1 = conv2d(input_, output_n=96, k_h=11, k_w=11, d_h=4, d_w=4, bias=0.0, activation_fc=self.default_act_fn, name='Conv1')\n",
    "        self.pool1 = tf.nn.max_pool(self.conv1_1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME', name='Pool1')\n",
    "        self.norm1 = batch_norm(self.pool1, epsilon=1e-5, momentum=0.9, train=True, name=\"Batch_Norm1\")\n",
    "        \n",
    "        self.conv2 = conv2d(self.norm1, output_n=256, k_h=5, k_w=5, d_h=1, d_w=1, bias=0.0, activation_fc=self.default_act_fn, name='Conv2')\n",
    "        self.pool2 = tf.nn.max_pool(self.conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME', name='Pool2')\n",
    "        self.norm2 = batch_norm(self.pool2, epsilon=1e-5, momentum=0.9, train=True, name=\"Batch_Norm2\")\n",
    "        \n",
    "        self.conv3 = conv2d(self.norm2, output_n=384, k_h=3, k_w=3, d_h=1, d_w=1, bias=0.0, activation_fc=self.default_act_fn, name='Conv3')\n",
    "\n",
    "        self.conv4 = conv2d(self.conv3, output_n=384, k_h=3, k_w=3, d_h=1, d_w=1, bias=0.0, activation_fc=self.default_act_fn, name='Conv4')\n",
    "        \n",
    "        self.conv5 = conv2d(self.conv4, output_n=256, k_h=3, k_w=3, d_h=1, d_w=1, bias=0.0, activation_fc=self.default_act_fn, name='Conv5')\n",
    "        self.pool5 = tf.nn.max_pool(self.conv5, ksize=[1,3,3,1], strides=[1,2,2,1], padding='VALID', name='Pool3')\n",
    "        \n",
    "        return self.pool5\n",
    "        \n",
    "        \n",
    "    def classifier(self, input_, keepratio):\n",
    "        self.flat1 = flatten(input_, name='flatten')\n",
    "        self.line1 = linear(self.flat1, 4096, name='fc_layer_4096')\n",
    "        self.drop2 = drop_out(self.line1, prob=keepratio)\n",
    "        self.line2 = linear(self.drop2, 4096, name='fc_layer_1000')\n",
    "        self.drop2 = drop_out(self.line2, prob=keepratio)\n",
    "        self.line3 = linear(self.drop2, self.class_n, name='fc_layer_'+str(self.class_n))\n",
    "        return self.line3\n",
    "    \n",
    "    def localizer(self, input_, alpha):\n",
    "        # Weight : Gaussian dist with standard deviation 0.01, and biases = 0\n",
    "        self.conv6 = conv2d(input_, output_n = 2048, k_h=1, k_w=1, d_h=1, d_w=1, bias=0.0, activation_fc=self.default_act_fn, name='Conv6')\n",
    "        print(self.conv6)\n",
    "        self.gap = tf.reduce_mean( self.conv6, [1,2] )\n",
    "        with tf.variable_scope(\"GAP\"):\n",
    "            self.gap_w = tf.get_variable(\"GAP_W\", shape=[2048, self.class_n],initializer=tf.random_normal_initializer(0., 0.01))\n",
    "            print(self.gap_w)\n",
    "\n",
    "        return tf.matmul(self.gap, self.gap_w)\n",
    "\n",
    "\n",
    "    def get_classmap(self, conv6, label):\n",
    "        conv6_resized = tf.image.resize_bilinear( conv6, [224, 224] )\n",
    "        with tf.variable_scope(\"GAP\", reuse=True):\n",
    "            label_w = tf.gather(tf.transpose(tf.get_variable(\"W\")), label)\n",
    "            print(label_w)\n",
    "            label_w = tf.reshape( label_w, [-1, 2048, 1] )\n",
    "            print(label_w)\n",
    "\n",
    "        conv6_resized = tf.reshape(conv6_resized, [-1, 224*224, 2048]) \n",
    "\n",
    "        classmap = tf.batch_matmul( conv6_resized, label_w )\n",
    "        classmap = tf.reshape( classmap, [-1, 224,224] )\n",
    "        return classmap\n",
    "\n",
    "    \n",
    "    def inference(self, input_, keepratio, alpha):\n",
    "        conv_layers = self.conv_layer(input_)\n",
    "        cls_pred = self.classifier(conv_layers, keepratio)\n",
    "        loc_pred = self.localizer(conv_layers, alpha)\n",
    "        return cls_pred, loc_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_size = 500\n",
    "channel_n = 1\n",
    "class_n = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, img_size, img_size, channel_n])\n",
    "y = tf.placeholder(tf.float32, shape=[None, class_n])\n",
    "alpha = tf.placeholder(tf.float32)\n",
    "keepratio = tf.placeholder(tf.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Transfer-Learning is Ready\n"
     ]
    }
   ],
   "source": [
    "model = STL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Conv6/act/add:0\", shape=(?, 15, 15, 2048), dtype=float32)\n",
      "Tensor(\"GAP/GAP_W/read:0\", shape=(2048, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "cls_pred, loc_pred = model.inference(x, keepratio, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost = wsl_cross_enropy(cls_pred, loc_pred, y, alpha)   # 처음엔 0.1로 시작하고서, 60 epochs 후에는 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
